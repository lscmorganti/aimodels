# ğŸ§  Mistral 7B com FastAPI + Docker + GPU

Projeto para rodar o modelo `Mistral-7B-Instruct-v0.2` localmente com aceleraÃ§Ã£o por GPU (RTX 3060 ou superior), usando quantizaÃ§Ã£o 4-bit e servindo uma API com FastAPI.

---

## ğŸ‡§ğŸ‡· InstruÃ§Ãµes em PortuguÃªs

### âœ… PrÃ©-requisitos

- Docker + NVIDIA Container Toolkit
- GPU com suporte a CUDA (ex: RTX 3060 com 12GB)
- Conta no [Hugging Face](https://huggingface.co/) com acesso ao repositÃ³rio `mistralai/Mistral-7B-Instruct-v0.2`

### ğŸ”§ Setup

1. Crie o arquivo `.env` com seu token Hugging Face:
   ```
   HUGGINGFACEHUB_API_TOKEN=hf_abc123...
   ```

2. Construa a imagem:
   ```bash
   make build
   ```

3. Suba o serviÃ§o:
   ```bash
   make up
   ```

4. Acesse a API:
   - DocumentaÃ§Ã£o Swagger: [http://localhost:8000/docs](http://localhost:8000/docs)
   - Endpoint `/generate`

### ğŸ§ª Exemplo de uso

```bash
http POST :8000/generate prompt="Seu prompt" max_tokens:=100
```

### ğŸ“¦ Cache dos modelos

O modelo Ã© baixado automaticamente e armazenado em:

```
/var/lib/docker/volumes/mistral_cache/_data
```

---

## ğŸ‡ºğŸ‡¸ English Instructions

### âœ… Requirements

- Docker + NVIDIA Container Toolkit installed
- GPU with CUDA support (e.g., RTX 3060 12GB)
- Hugging Face account with access to [`mistralai/Mistral-7B-Instruct-v0.2`](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2)

### ğŸ”§ Setup

1. Create a `.env` file with your token:
   ```
   HUGGINGFACEHUB_API_TOKEN=hf_abc123...
   ```

2. Build the container:
   ```bash
   make build
   ```

3. Start the service:
   ```bash
   make up
   ```

4. Access the API:
   - Swagger docs: [http://localhost:8000/docs](http://localhost:8000/docs)
   - Endpoint: `/generate`

### ğŸ§ª Example request

```bash
http POST :8000/generate prompt="Your prompt" max_tokens:=100
```

### ğŸ“¦ Model cache

Model files are downloaded and stored in the volume:

```
/var/lib/docker/volumes/mistral_cache/_data
```

---

## ğŸ“‚ Estrutura do projeto

```
.
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ main.py         # FastAPI app
â”‚   â””â”€â”€ model.py        # Carrega e executa o modelo
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yaml
â”œâ”€â”€ makefile
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env
```

---

## ğŸ‘¨â€ğŸ’» Autor

Lucas Morganti â€” projeto local com uso de LLM acelerada por GPU com infraestrutura Docker.